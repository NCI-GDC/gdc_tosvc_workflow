### To generate a new purecn normaldb
* notes used to make this
  * https://wiki.uchicago.edu/pages/viewpage.action?spaceKey=CDIS&title=Tumor-Only+Filtering+Workflow+Manual
1. clone repo
   * `git@github.com:NCI-GDC/gdc_tosvc_workflow.git`
1. cd `gdc_tosvc_workflow/normaldb`
1. The entrypoint for normaldb generation is `etl.cwl` (extract, transform, load)
1. Example inputs are:
   * `etl.nexterarapidcaptureexomev1.2.cptac-3.hg38.yml`
   * `etl.sureselectxthumanallexonv5,16.tcga-meso.hg38.yml`
   * `etl.xgenexomeresearchpanelv1.0.organoid-pancreatic.hg38.yml`
1. input fields that must be modified for each new kit:
   * `bam_uuids`
   * `bam_sizes`
   * `bam_index_uuids`
   * `bam_index_sizes`
   * `project_id`
   * `target_capture_kit`
1. save, git commit and push
1. rsync repo to a VM
   * `rsync -av -progress gdc_tosvc_workflow ${VM}/mnt/scratch/`
1. run workflow
   * `cd /mnt/scratch/`
   * `mkdir run && cd run/`
   * ```bash
     nohup cwltool --debug --cachedir $(pwd)/cache/ --tmpdir-prefix $(pwd)/tmp/ /mnt/scratch/gdc_tosvc_workflow/normaldb/etl.cwl /mnt/scratch/gdc_tosvc_workflow/normaldb/etl.xgenexomeresearchpanelv1.0.organoid-pancreatic.hg38.yml &
     ```
1. output will be 4 files. in the above example:
   * `interval_weights.xgenexomeresearchpanelv1.0.organoid-pancreatic.hg38.png`
   * `interval_weights.xgenexomeresearchpanelv1.0.organoid-pancreatic.hg38.txt`
   * `low_coverage_targets.xgenexomeresearchpanelv1.0.organoid-pancreatic.hg38.bed`
   * `normalDB.xgenexomeresearchpanelv1.0.organoid-pancreatic.hg38.rds`
1. files to be used by workflows
   * `interval_weights.*.hg38.txt`
   * `normalDB.*.rds`
1. these files should be placed in an object store
   * as with files generated by https://github.com/NCI-GDC/gdc_bed_liftover/
   * I use `awscli`. Our ceph/cleversafe store only works with older versions of `awscli` (Sean Sullivan knows the details).
     * The virtualenv I use:
       * `mkvirtualenv --python /usr/bin/python3 p3`
       * `pip install awscli==1.10.50`
       * `awscli` config:
       * `~/.aws/config` [0]
       * `~/.aws/credentials` [1]
       * example object store write:
         ```bash
         aws s3 cp --profile ceph --endpoint-url http://gdc-cephb-objstore.osdc.io/ interval_weights.*.hg38.txt s3://capture-kits/auto/
         aws s3 cp --profile ceph --endpoint-url http://gdc-cephb-objstore.osdc.io/ normalDB.*.rds s3://capture-kits/auto/
         ```
1. these files should be made available to biowcs and entered into indexd
   1. `git clone git@github.com:NCI-GDC/gdc_bed_liftover.git`
   1. Modify `gdc_bed_liftover/map_file_kit.ods` with `oocalc`
      1. Find the row for the `bait` [2] file of the target capture kit, and populate:
         * `normaldb_rds` (column N)
         * `normaldb_txt` (column O)
         * one UUID values will need to be generated for each column, followed by a comma, and then the filename.
         * example kit: `xGen Exome Research Panel v1.0`
         * Save `map_file_kit.ods`
         * Select `File > Save As...` and select `Text CSV (csv)` from the menu above the `Save` button. Then click `Save`. If a dialog asks if you are sure, select `Use Text CSV Format`. Then, `Character set: Unicode (UTF-8)`, `Field delimiter: {Tab}`, `String delimiter: "`. Check `Save cell content as shown` and `Quote all text cells`. Click `OK`.
         * Quit `oocalc`
         * Use the terminal to `mv map_file_kit.csv map_file_kit.tsv`.
         * `git commit -am "updated with new normaldb"`
   1. https://github.com/NCI-GDC/gdc_bed_liftover/blob/master/gdc_bed_liftover/create_enum_map.py
     * creates biowcs `kit_enum_map.json`
     * creates `indexd_records.json` for bioindexd update
   1. `rsync -av --progress gdc_bed_liftover ${VM}:`
   1. from `${VM}`
   1. `python ~/gdc_bed_liftover/gdc_bed_liftover/create_enum_map.py --tsvfile ~/gdc_bed_liftover/gdc_bed_liftover/map_file_kit.tsv --existing_enum_map_json ~/gdc_bed_liftover/gdc_bed_liftover/kit_enum_map.json --kit_dir /mnt/scratch/run/ --s3_url_base s3://ceph.service.consul/capture-kits/auto/`
      * two files will be output: `kit_enum_map.json` and `indexd_records.json`
      * `kit_enum_map.json` will be placed in two repos. The first for record keeping, the second for biowcs deployment:
        1. `cp kit_enum_map.json ~/gdc_bed_liftover/gdc_bed_liftover/ && cd ~/gdc_bed_liftover/ && git commit -am "update kit_enum_map.json" && git push && cd -`
        1. `git clone git@github.com:NCI-GDC/biowcs.git && cd ~/biowcs && git checkout -b chore/update-kit_enum_map && cp -a ~/kit_enum_map.json ~/biowcs/biowcs/dynamic/resources/kit_enum_map.json && git commit -am "update kit_enum_map.json && git push --set-upstream origin chore/update-kit_enum_map" && cd -`
               *. merge `chore/update-kit_enum_map` to `master` at http://github.com/NCI-GDC/biowcs
      * `indexd_records.json` will be placed in one repo for record keeping:
        1. `cp indexd_records.json ~/gdc_bed_liftover/gdc_bed_liftover/ && cd ~/gdc_bed_liftover/ && git commit -am "update indexd_records.json" && git push && cd -`

1. update indexd records on dev and prod bioindexd:
   1. transfer `indexd_records.json` to a VM
   1. on VM copy to `${HOME}` https://github.com/NCI-GDC/workflow-deployment/blob/master/bamfastq_align/set_indexd_records.py
     * see https://github.com/NCI-GDC/workflow-deployment/blob/master/bamfastq_align/README.md for requirements
   1. `python set_indexd_records.py -c indexd.json -i indexd_records.json`


[0]:
```bash
(p3) ubuntu@jeremiah-node001:~$ cat .aws/config 
[default]
s3 =
  max_concurrent_requests = 1
  max_queue_size = 10000
  multipart_threshold = 5GB
  multipart_chunksize = 5GB

[ceph]
s3 =
  max_concurrent_requests = 1
  max_queue_size = 10000
  multipart_threshold = 5GB
  multipart_chunksize = 5GB

[cleversafe]
s3 =
  max_concurrent_requests = 1
  max_queue_size = 10000
  multipart_threshold = 5GB
  multipart_chunksize = 5GB
```
[1]:
```bash
(p3) ubuntu@jeremiah-node001:~$ cat .aws/credentials
[default]
aws_access_key_id = xxx
aws_secret_access_key = xxx
[ceph]
aws_access_key_id = xxx
aws_secret_access_key = xxx
[cleversafe]
aws_secret_access_key = xxx
aws_access_key_id = xxx
[aws_secure]
aws_access_key_id = xxx
aws_secret_access_key = xxx
```

[2]
https://github.com/lima1/PureCN/blob/51718042e39fe2d24d76a2b6c9139c3c242f4a34/vignettes/PureCN.Rnw#L128-L133
```
Usually the manufacturer provides two files: the baits file containing the coordinates of the actual capture baits, and the target file containing the coordinates of the actual regions we want to capture. We recommend to use the baits file (and recognize the confusing nomenclature that we follow due to convention in established tools).
```
also see
https://github.com/lima1/PureCN/blob/51718042e39fe2d24d76a2b6c9139c3c242f4a34/vignettes/Quick.Rmd#L109
